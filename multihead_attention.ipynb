{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **BUILDING MULTOHEAD ATTENTION FROM SCRATCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def multihead_attention(q, k, v, num_heads=2):\n",
    "    d_model = q.shape[-1]\n",
    "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "    depth = d_model // num_heads\n",
    "\n",
    "    # Split heads\n",
    "    def split_heads(x):\n",
    "        return x.reshape(x.shape[0], -1, num_heads, depth).transpose(0, 2, 1, 3)\n",
    "\n",
    "    q_heads = split_heads(q)\n",
    "    k_heads = split_heads(k)\n",
    "    v_heads = split_heads(v)\n",
    "\n",
    "    # Scaled dot-product attention\n",
    "    scores = np.matmul(q_heads, k_heads.transpose(0, 1, 3, 2)) / np.sqrt(depth)\n",
    "    weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
    "    weights /= np.sum(weights, axis=-1, keepdims=True)\n",
    "\n",
    "    context = np.matmul(weights, v_heads)\n",
    "\n",
    "    # Combine heads\n",
    "    context = context.transpose(0, 2, 1, 3).reshape(context.shape[0], -1, d_model)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
